from app.core.config import settings
from app.core.llm_config import get_conversation_chain
from app.services.conversa_service import consultar_resposta_banco_service, salvar_conversa_service
from app.services.curso_service import sugerir_curso_service
from app.models.pydantic_models import RespostaOutputModel, CursoSugestaoModel, PerguntaInputModel
import urllib.parse
import re
import hashlib
from app.core.db_config import get_db_collection # Para verificar hash no BD

# Fun√ß√µes adaptadas do script original

def identificar_tema_link(pergunta: str) -> Optional[str]:
    for tema, link in settings.TEMAS_PARA_LINKS.items():
        if tema.lower() in pergunta.lower():
            return link
    return None

def verificar_conhecimento_interno_llm(pergunta: str, resposta: str, chat_chain) -> bool:
    # Esta fun√ß√£o √© complexa de replicar 100% sem o LLM exato e contexto do script original.
    # No script original, ele usa o LLM para perguntar a si mesmo se a resposta √© de conhecimento interno.
    # Para uma API, isso pode ser simplificado ou assumir que respostas do LLM sem consulta a BD/Docs s√£o "conhecimento interno".
    # Por ora, vamos simular uma l√≥gica mais simples ou pular essa verifica√ß√£o espec√≠fica via LLM.
    # No script original: 
    # prompt_verificacao = f"""Esta resposta √© do seu conhecimento interno? Responda APENAS com SIM ou N√ÉO:\nPergunta: {pergunta}\nResposta: {resposta}"""
    # verificacao = openrouter_llm.generate(prompt_verificacao).text.strip()
    # if verificacao.upper() == "SIM": return True
    # return False
    # Como alternativa, podemos verificar se a resposta est√° no buffer de mem√≥ria recente do chat_chain
    if chat_chain and chat_chain.memory:
        buffer_str = str(chat_chain.memory.buffer_as_str) # ou buffer_as_messages
        if resposta in buffer_str: # Simplifica√ß√£o, pode n√£o ser precisa
            return True
    return False # Defaulting to false if not found in recent memory or no reliable check

def verificar_origem_resposta_service(pergunta: str, resposta_bruta: str, chat_chain) -> str:
    print(f'\nVerificando origem da resposta para: {pergunta}')
    
    # 1. Verificar na mem√≥ria de contexto do chat_chain (se aplic√°vel e implementado no chain)
    if chat_chain and chat_chain.memory:
        # A mem√≥ria do ConversationBufferMemory guarda o hist√≥rico.
        # Se a resposta exata estiver l√°, pode ser considerada da mem√≥ria.
        # Esta √© uma verifica√ß√£o simplificada.
        if resposta_bruta in str(chat_chain.memory.buffer): # Convertendo buffer para string para busca
            origem = "Mem√≥ria de Contexto da Conversa Atual"
            print(f'Detectado: {origem}')
            return origem

    # 2. Verificar no Banco de Dados (MongoDB) por hash ou regex (similar ao script original)
    collection = get_db_collection()
    if collection:
        hash_r = hashlib.md5(resposta_bruta.encode()).hexdigest()
        # Regex para buscar uma por√ß√£o inicial da resposta, caso o hash n√£o bata por pequenas varia√ß√µes
        resposta_regex_escaped = re.escape(resposta_bruta[:50]) 
        
        # Tenta encontrar por hash OU por uma parte da resposta (regex)
        # Adicionar filtro de persona/usu√°rio se implementado no futuro
        resultado_db = collection.find_one({
            "$or": [
                {"hash_resposta": hash_r},
                {"resposta": {"$regex": resposta_regex_escaped, "$options": "i"}}
            ]
        })
        if resultado_db:
            origem_db = resultado_db.get('origem', 'Origem Desconhecida no BD')
            origem = f"Fonte Prim√°ria - Banco de Dados ({origem_db})"
            print(f'Detectado: {origem}')
            return origem

    # 3. Verificar se √© sobre um tema com documenta√ß√£o oficial mapeada
    link_doc = identificar_tema_link(pergunta)
    if link_doc:
        origem = f"Fonte Secund√°ria - Documenta√ß√£o Oficial ({link_doc})"
        print(f'Detectado: {origem}')
        return origem

    # 4. Tentar verificar se √© conhecimento interno do LLM (heur√≠stica)
    # Esta parte √© mais complexa de replicar fielmente sem chamar o LLM para se auto-avaliar.
    # A fun√ß√£o `verificar_conhecimento_interno_llm` √© uma tentativa.
    # if verificar_conhecimento_interno_llm(pergunta, resposta_bruta, chat_chain):
    #     origem = "Conhecimento Interno do LLM"
    #     print(f'Detectado: {origem}')
    #     return origem
    # Por ora, se n√£o veio do BD ou Docs, e √© uma resposta do LLM, vamos classificar como "Nova Gera√ß√£o pelo LLM"
    # ou "Conhecimento Interno do LLM" se a heur√≠stica acima for mais robusta.

    # 5. Se nenhuma das anteriores, assume-se que foi gerada pelo LLM (potencialmente com consulta web se o LLM tiver essa capacidade)
    # ou √© uma nova gera√ß√£o baseada no prompt.
    # O script original adicionava um link de pesquisa Google como "Fonte Terci√°ria".
    # url_pesquisa = f"https://www.google.com/search?q={urllib.parse.quote(pergunta)}"
    # origem = f"Fonte Terci√°ria - Nova Gera√ß√£o pelo LLM (ref: {url_pesquisa})"
    # Simplificando para a API:
    origem = "Nova Gera√ß√£o pelo LLM"
    print(f'Detectado: {origem} (ou conhecimento interno n√£o capturado pelas heur√≠sticas anteriores)')
    return origem

def processar_pergunta_service(pergunta_input: PerguntaInputModel) -> RespostaOutputModel:
    pergunta_texto = pergunta_input.texto_pergunta.strip()
    
    if not pergunta_texto:
        # Comportamento do script original para pergunta vazia
        pergunta_texto = "Quem √© vc ? Apresente-se."

    # 0. Tentar consultar resposta no banco antes de chamar o LLM
    resposta_do_banco = consultar_resposta_banco_service(pergunta_texto)
    sugestao_curso_obj = sugerir_curso_service(pergunta_texto)
    links_doc = []
    link_tema = identificar_tema_link(pergunta_texto)
    if link_tema:
        links_doc.append(link_tema)

    if resposta_do_banco:
        print("Resposta encontrada diretamente no banco de dados.")
        origem_final = "Fonte Prim√°ria - Banco de Dados (Consulta Direta por Similaridade)"
        # Adicionar sugest√£o de curso e links de documenta√ß√£o se aplic√°vel
        resposta_final_formatada = f"{resposta_do_banco}"
        if sugestao_curso_obj:
             resposta_final_formatada += f"\n\nüéì **Sugest√£o de curso: [{sugestao_curso_obj.nome_curso}]({sugestao_curso_obj.link_curso})**"
        if links_doc:
            resposta_final_formatada += "\n\nüîó **Documenta√ß√£o Relacionada:**"
            for link in links_doc:
                resposta_final_formatada += f"\n- {link}"
        
        # Salvar a "nova" pergunta com a resposta do banco para fins de log/frequ√™ncia, se desejado.
        # salvar_conversa_service(pergunta_texto, resposta_do_banco, origem_final) # Opcional, pois j√° est√° no banco
        
        return RespostaOutputModel(
            pergunta_original=pergunta_texto,
            texto_resposta=resposta_final_formatada,
            origem_resposta=origem_final,
            sugestao_curso=sugestao_curso_obj,
            links_documentacao=links_doc
        )

    # 1. Preparar o LLM e o prompt
    # Tenta OpenRouter primeiro, fallback para Groq se configurado no llm_config
    try:
        chat_chain = get_conversation_chain(llm_preference="openrouter") 
    except Exception as e:
        print(f"Falha ao obter chat_chain: {e}")
        # Tratar erro de LLM n√£o dispon√≠vel
        return RespostaOutputModel(
            pergunta_original=pergunta_texto,
            texto_resposta="Desculpe, o servi√ßo de IA est√° temporariamente indispon√≠vel.",
            origem_resposta="Erro Interno - LLM Inacess√≠vel",
            sugestao_curso=sugestao_curso_obj, # Ainda pode sugerir curso
            links_documentacao=links_doc
        )

    # O system_prompt √© parte da configura√ß√£o do settings e usado no template do ConversationChain
    # ou pode ser injetado aqui se o template for mais simples.
    # No script original, o system_prompt era concatenado com a pergunta.
    # A ConversationChain com PromptTemplate j√° lida com a formata√ß√£o do hist√≥rico e input.
    prompt_para_llm = f"{settings.SYSTEM_PROMPT}\n\nAluno: {pergunta_texto}\nAssistente:"
    # A label "Aluno:" e "Assistente:" ajuda o LLM a entender o papel.
    # O template da chain √© "{history}\nHumano: {input}\nAssistente:"
    # Ent√£o o input para chain.run deve ser apenas a pergunta do usu√°rio, 
    # e o system_prompt j√° est√° no template ou no llm.
    # Ajuste: O system_prompt deve ser parte do contexto inicial, n√£o repetido a cada input se a mem√≥ria j√° o cont√©m.
    # Para a primeira mensagem ou se a mem√≥ria n√£o for persistente entre chamadas de API (o que √© o caso aqui por padr√£o)
    # √© bom incluir o system prompt.

    # Se a ConversationChain j√° tem um template que inclui o system_prompt, 
    # ent√£o o input para `run` seria apenas `pergunta_texto`.
    # Vamos assumir que o `get_conversation_chain` e seu `PromptTemplate` n√£o incluem o `SYSTEM_PROMPT` diretamente,
    # ent√£o o concatenamos aqui como parte do `input`.
    # Revis√£o: O template √© "{history}\nHumano: {input}\nAssistente:". 
    # O `SYSTEM_PROMPT` deve ser a primeira mensagem do `history` ou uma instru√ß√£o para o LLM.
    # Para ConversationChain, o `system_message` pode ser setado na mem√≥ria ou no LLM.
    # Se n√£o, a forma mais simples √© prefixar o input.

    # Se a mem√≥ria for nova a cada chamada (t√≠pico de API stateless):
    if not chat_chain.memory.buffer: # Se a mem√≥ria est√° vazia
        # Adiciona o system prompt como uma mensagem inicial do sistema ou do AI.
        # Isso n√£o √© padr√£o para ConversationBufferMemory, que espera tuplas (humano, ia).
        # Uma forma √© passar o system_prompt como parte do primeiro input do usu√°rio.
        input_llm = f"{settings.SYSTEM_PROMPT}\n\nPergunta espec√≠fica: {pergunta_texto}"
    else:
        input_llm = pergunta_texto

    print(f"Enviando para LLM: {input_llm[:200]}...") # Log do input
    resposta_bruta_llm = chat_chain.run(input_llm)
    print(f"Resposta bruta do LLM: {resposta_bruta_llm[:200]}...")

    # 2. Verificar a origem da resposta do LLM
    origem_resposta_llm = verificar_origem_resposta_service(pergunta_texto, resposta_bruta_llm, chat_chain)

    # 3. Formatar a resposta final, adicionando sugest√£o de curso e links
    resposta_final_formatada = f"{resposta_bruta_llm}"
    
    if sugestao_curso_obj:
        resposta_final_formatada += f"\n\nüéì **Sugest√£o de curso: [{sugestao_curso_obj.nome_curso}]({sugestao_curso_obj.link_curso})**"
    
    # Adicionar links de documenta√ß√£o se a origem n√£o for j√° de documenta√ß√£o
    if links_doc and "Documenta√ß√£o Oficial" not in origem_resposta_llm:
        resposta_final_formatada += "\n\nüîó **Documenta√ß√£o Relacionada:**"
        for link in links_doc:
            resposta_final_formatada += f"\n- {link}"
    elif "Documenta√ß√£o Oficial" in origem_resposta_llm: # Se a origem j√° √© doc, o link j√° est√° l√°
        pass 

    # Adicionar link de pesquisa gen√©rico se a origem for "Nova Gera√ß√£o pelo LLM"
    if "Nova Gera√ß√£o pelo LLM" in origem_resposta_llm:
        url_pesquisa = f"https://www.google.com/search?q={urllib.parse.quote(pergunta_texto)}"
        resposta_final_formatada += f"\n\nüîó **Para mais informa√ß√µes, consulte: {url_pesquisa}**"

    # 4. Salvar a conversa no MongoDB
    salvar_conversa_service(pergunta_texto, resposta_final_formatada, origem_resposta_llm)

    return RespostaOutputModel(
        pergunta_original=pergunta_texto,
        texto_resposta=resposta_final_formatada,
        origem_resposta=origem_resposta_llm,
        sugestao_curso=sugestao_curso_obj,
        links_documentacao=links_doc
    )

